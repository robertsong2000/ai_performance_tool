#!/usr/bin/env python3
"""
ç®€å•çš„æ¼”ç¤ºè„šæœ¬ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨LM Studioæ€§èƒ½æµ‹è¯•å·¥å…·
"""

from lm_studio_tester import LMStudioPerformanceTester, get_test_prompts


def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ä½¿ç”¨æ–¹æ³•"""
    print("ğŸ¯ LM Studioæ€§èƒ½æµ‹è¯•å·¥å…·æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
    tester = LMStudioPerformanceTester()
    
    # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
    print("1ï¸âƒ£ æ£€æŸ¥æœåŠ¡å™¨è¿æ¥...")
    if not tester.check_server_status():
        print("âŒ æ— æ³•è¿æ¥åˆ°LM Studioï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨")
        return
    
    # å•æ¬¡æµ‹è¯•
    print("\n2ï¸âƒ£ æ‰§è¡Œå•æ¬¡æ¨ç†æµ‹è¯•...")
    prompt = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚"
    metrics = tester.single_inference_test(prompt, max_tokens=50)
    
    if metrics:
        print(f"âœ… æµ‹è¯•æˆåŠŸ!")
        print(f"   å“åº”æ—¶é—´: {metrics.response_time:.3f}ç§’")
        print(f"   ç”Ÿæˆé€Ÿåº¦: {metrics.tokens_per_second:.2f} tokens/ç§’")
        print(f"   ç”Ÿæˆå†…å®¹é•¿åº¦: {metrics.completion_tokens} tokens")
    
    # æ‰¹é‡æµ‹è¯•
    print("\n3ï¸âƒ£ æ‰§è¡Œæ‰¹é‡æµ‹è¯•...")
    test_prompts = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "è§£é‡Šæ·±åº¦å­¦ä¹ çš„æ¦‚å¿µã€‚",
        "Pythonæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"
    ]
    
    batch_results = tester.batch_inference_test(test_prompts, max_tokens=80)
    print(f"âœ… æ‰¹é‡æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸ {len(batch_results)}/{len(test_prompts)} ä¸ªè¯·æ±‚")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n4ï¸âƒ£ ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
    tester.print_report()
    
    # ä¿å­˜ç»“æœ
    print("\n5ï¸âƒ£ ä¿å­˜æµ‹è¯•ç»“æœ...")
    tester.save_detailed_results("demo_results.json")
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print("ğŸ’¡ æç¤º: è¿è¡Œ 'python visualizer.py --file demo_results.json' æŸ¥çœ‹å¯è§†åŒ–æŠ¥å‘Š")


def demo_custom_test():
    """æ¼”ç¤ºè‡ªå®šä¹‰æµ‹è¯•"""
    print("\nğŸ”§ è‡ªå®šä¹‰æµ‹è¯•æ¼”ç¤º")
    print("-" * 30)
    
    # ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°åˆ›å»ºæµ‹è¯•å™¨
    tester = LMStudioPerformanceTester(
        base_url="http://localhost:1234",
        model_name="custom-model"  # å¯ä»¥æŒ‡å®šç‰¹å®šæ¨¡å‹
    )
    
    if not tester.check_server_status():
        print("âŒ æœåŠ¡å™¨è¿æ¥å¤±è´¥")
        return
    
    # è‡ªå®šä¹‰æµ‹è¯•å‚æ•°
    custom_prompt = "ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—ä¸¤ä¸ªæ•°çš„æœ€å¤§å…¬çº¦æ•°ã€‚"
    
    print("ğŸ§ª æ‰§è¡Œè‡ªå®šä¹‰å‚æ•°æµ‹è¯•...")
    metrics = tester.single_inference_test(
        prompt=custom_prompt,
        max_tokens=200,  # æ›´é•¿çš„è¾“å‡º
        temperature=0.3  # æ›´ç¡®å®šæ€§çš„è¾“å‡º
    )
    
    if metrics:
        print(f"âœ… è‡ªå®šä¹‰æµ‹è¯•å®Œæˆ")
        print(f"   å“åº”æ—¶é—´: {metrics.response_time:.3f}ç§’")
        print(f"   ç”Ÿæˆé€Ÿåº¦: {metrics.tokens_per_second:.2f} tokens/ç§’")


def demo_performance_comparison():
    """æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”æ¼”ç¤º")
    print("-" * 30)
    
    tester = LMStudioPerformanceTester()
    
    if not tester.check_server_status():
        print("âŒ æœåŠ¡å™¨è¿æ¥å¤±è´¥")
        return
    
    # æµ‹è¯•ä¸åŒé•¿åº¦çš„è¾“å‡º
    test_cases = [
        {"name": "çŸ­å›ç­”", "max_tokens": 50, "prompt": "ç®€å•å›ç­”ï¼šä»€ä¹ˆæ˜¯AIï¼Ÿ"},
        {"name": "ä¸­ç­‰å›ç­”", "max_tokens": 150, "prompt": "è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"},
        {"name": "é•¿å›ç­”", "max_tokens": 300, "prompt": "è¯·è¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†å²å’Œåº”ç”¨é¢†åŸŸã€‚"}
    ]
    
    results = {}
    
    for case in test_cases:
        print(f"ğŸ§ª æµ‹è¯• {case['name']} (max_tokens={case['max_tokens']})")
        metrics = tester.single_inference_test(case['prompt'], case['max_tokens'])
        
        if metrics:
            results[case['name']] = {
                'response_time': metrics.response_time,
                'tokens_per_second': metrics.tokens_per_second,
                'completion_tokens': metrics.completion_tokens
            }
            print(f"   âœ… å®Œæˆ - {metrics.response_time:.3f}s, {metrics.tokens_per_second:.2f} TPS")
        else:
            print(f"   âŒ å¤±è´¥")
    
    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print(f"{'æµ‹è¯•ç±»å‹':<10} {'å“åº”æ—¶é—´':<10} {'ç”Ÿæˆé€Ÿåº¦':<12} {'Tokenæ•°':<8}")
    print("-" * 45)
    
    for name, data in results.items():
        print(f"{name:<10} {data['response_time']:<10.3f} {data['tokens_per_second']:<12.2f} {data['completion_tokens']:<8}")


if __name__ == "__main__":
    try:
        # åŸºæœ¬æ¼”ç¤º
        demo_basic_usage()
        
        # è‡ªå®šä¹‰æµ‹è¯•æ¼”ç¤º
        demo_custom_test()
        
        # æ€§èƒ½å¯¹æ¯”æ¼”ç¤º
        demo_performance_comparison()
        
        print("\nğŸŠ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
        print("ğŸ“š æ›´å¤šåŠŸèƒ½è¯·å‚è€ƒ README.md æˆ–è¿è¡Œ 'python main.py --help'")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿LM Studioå·²å¯åŠ¨å¹¶åŠ è½½äº†æ¨¡å‹")